<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Processing Background Shapes: From Radar Signals to ML Infrastructure</title>
    <meta name="description" content="How my PhD in radar signal processing translated into building distributed ML systems at Oracle HeatWave.">
    <meta property="og:image" content="https://anyech.github.io/jingxiao-cai-blog/og-image.png">
    <meta name="twitter:image" content="https://anyech.github.io/jingxiao-cai-blog/og-image.png">
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <header>
        <nav>
            <a href="index.html">← Back to Home</a>
        </nav>
    </header>

    <main>
        <article>
            <header class="post-header">
                <h1>Processing Background Shapes: From Radar Signals to ML Infrastructure</h1>
                <p class="post-date">February 19, 2026</p>
            </header>

            <div class="tldr-box">
                <strong>TL;DR:</strong> My PhD in radar signal processing taught me to extract meaningful signals from noisy backgrounds—a skill that directly translated to building scalable ML infrastructure at Oracle. The same mathematical intuition behind adaptive sampling and turbulence detection now powers HeatWave ML's distributed computing layer.
            </div>

            <section>
                <h2>The Unexpected Bridge</h2>
                <p>When I tell people I have a PhD in radar signal processing and now work on ML infrastructure, they often see it as a career pivot. But internally, it feels like a direct continuation—the mathematics of extracting signals from noise is remarkably similar whether you're processing radar returns or training distributed machine learning models.</p>
                <p>My dissertation at the University of Oklahoma focused on adaptive sampling for radar systems, specifically detecting turbulence in atmospheric conditions. The core problem: how do you extract meaningful patterns from overwhelmingly noisy data streams, in real-time, with limited computational resources?</p>
                <p>Turns out, that's exactly the same challenge we face in distributed ML training.</p>
            </section>

            <section>
                <h2>The Mathematics of Noise</h2>
                <p>Radar signal processing is fundamentally about probability and statistical modeling. When a radar pulse bounces off objects in the sky, it returns with various noise components:</p>
                <ul>
                    <li><strong>Thermal noise</strong> — the inherent randomness of electron movement</li>
                    <li><strong>Clutter</strong> — unwanted echoes from ground, weather, or birds</li>
                    <li><strong>Jamming</strong> — intentional interference</li>
                </ul>
                <p>The art is separating the signal (an aircraft, a storm cell) from all this noise. This requires sophisticated filtering, adaptive algorithms, and understanding of signal statistics.</p>
                <p>In ML infrastructure, we face a parallel problem: distinguishing meaningful gradient updates from noise in distributed training, identifying real performance bottlenecks versus false positives in profiling, and efficiently allocating compute across thousands of nodes.</p>
            </section>

            <section>
                <h2>From Adaptive Sampling to Adaptive Computation</h2>
                <p>My PhD work on adaptive sampling involved dynamically adjusting sampling rates based on signal characteristics. When the signal was "interesting" (high variance, potential targets), we'd sample at full rate. When conditions were stable, we could throttle back.</p>
                <p>At Oracle, this intuition maps directly to how we handle computation in HeatWave ML. Consider gradient computation in distributed training:</p>
                <ul>
                    <li>Not all gradients are equally important</li>
                    <li>Some layers converge faster than others</li>
                    <li>Communication overhead varies dramatically based on model architecture</li>
                </ul>
                <p>The same mathematical framework I used to decide "when to sample" now helps decide "when to communicate," "which computations to prioritize," and "how to balance compute against data movement."</p>
            </section>

            <section>
                <h2>Building Zero-Copy ML Systems</h2>
                <p>One of my proudest contributions at Oracle was implementing zero-copy protocols for ML training data transfer. Traditional approaches buffer data multiple times—disk to memory, memory to GPU, GPU back to CPU for aggregation. Each buffer copy adds latency and memory pressure.</p>
                <p>The insight came from radar processing: we always tried to minimize "processing latency" because by the time you've fully processed a radar return, the aircraft has moved. The same urgency applies to ML training loops.</p>
                <p>Using techniques similar to radar's "pipelined processing," we implemented direct memory transfer paths that eliminate intermediate buffering. This reduced memory overhead by 40% for large-scale Dask+ML workloads and improved throughput significantly.</p>
            </section>

            <section>
                <h2>The FedRAMP Connection</h2>
                <p>Working on FedRAMP-authorized systems added another layer of complexity. Radar systems operate under strict real-time constraints—there's no "retry later" when detecting aircraft. Similarly, enterprise ML systems have reliability requirements that go beyond typical research prototypes.</p>
                <p>My experience with radar's fault-tolerant design translated well: redundant computation paths, graceful degradation, and comprehensive logging. When you're building systems that need FedRAMP authorization, you can't treat reliability as an afterthought.</p>
            </section>

            <section>
                <h2>What I Learned</h2>
                <p>Looking back, the PhD wasn't just about the specific technical skills—it was about developing intuition for:</p>
                <ul>
                    <li><strong>Working with noisy data</strong> — ML training is fundamentally about extracting signal from noise in gradients, loss surfaces, and hyperparameter spaces</li>
                    <li><strong>Real-time constraints</strong> — understanding latency vs. throughput tradeoffs at a fundamental level</li>
                    <li><strong>Mathematical modeling</strong> — knowing when to apply statistical methods vs. deterministic approaches</li>
                    <li><strong>System-level thinking</strong> — radar systems are inherently distributed, requiring coordination across components</li>
                </ul>
                <p>If you're considering a career transition from academia to industry, my advice: look for the underlying mathematical and systems-level similarities, not just the surface-level topic differences. The patterns transfer more than you'd expect.</p>
            </section>

            <section>
                <h2>What's Next</h2>
                <p>After building HeatWave ML infrastructure at Oracle, I'm now exploring the next frontier: how to make distributed ML training even more efficient, particularly for the emerging class of large models that push the boundaries of memory and communication.</p>
                <p>The radar background taught me that constraints drive innovation. When you can't simply "throw more compute" at a problem (whether due to real-time requirements or hardware limitations), you develop creative solutions that often outperform brute-force approaches.</p>
                <p>I'm excited to continue applying these principles to next-generation ML systems.</p>
            </section>

            <hr>

            <div class="author-bio">
                <h3>About the Author</h3>
                <p><strong>Dr. JCai</strong> is a Principal Member of Technical Staff at Oracle, where he works on HeatWave ML infrastructure. He holds a PhD in Electrical Engineering from the University of Oklahoma, focusing on radar signal processing and adaptive sampling. Previously, he built distributed ML systems handling 200+ node Dask clusters and implemented zero-copy protocols for ML training data transfer. When not optimizing ML infrastructure, he's likely thinking about cars, playing WoW, or finding better ways to extract signals from noise.</p>
            </div>
        </article>
    </main>

    <footer>
        <p>&copy; 2026 jingxiao-cai. Built with OpenClaw.</p>
    </footer>
</body>
</html>
